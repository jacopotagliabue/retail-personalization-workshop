{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instrumental-cabin",
   "metadata": {},
   "source": [
    "# SESSIONS ARE ALL YOU NEED\n",
    "### Workshop on e-commerce personalization\n",
    "\n",
    "This notebook showcases with working code the main ideas of our ML-in-retail workshop from June lst, 2021 at MICES (https://mices.co/). Please refer to the README in the repo for a bit of context!\n",
    "\n",
    "While the code below is (well, should be!) fully functioning, please note we aim for functions which are pedagogically useful, more than terse code per se: it should be fairly easy to take these ideas and refactor the code to achieve more speed, better re-usability etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-superior",
   "metadata": {},
   "source": [
    "_If you want to use Google Colab, you can uncomment this cell:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need requirements....\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "# #from google.colab import drive\n",
    "#drive.mount('/content/drive',force_remount=True)\n",
    "#%cd drive/MyDrive/path_to_directory_containing_train_folder\n",
    "#LOCAL_FOLDER = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-diving",
   "metadata": {},
   "source": [
    "## Basic import and some global vars to know where data is!\n",
    "\n",
    "Here we import the libraries we need and set the working folders - make sure your current python interpreter has all the dependencies installed. If you want to use the same real-world data as I'm using, please download the open dataset you find at: https://github.com/coveooss/SIGIR-ecom-data-challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import choice\n",
    "import time\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import Counter,defaultdict\n",
    "# viz stuff\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image \n",
    "# gensim stuff for prod2vec\n",
    "import gensim  # gensim > 4\n",
    "from gensim.similarities.annoy import AnnoyIndexer\n",
    "# keras stuff for auto-encoder\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "import hashlib\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_FOLDER = '/Users/jacopotagliabue/Documents/data_dump/train'  # where is the dataset stored?\n",
    "N_ROWS = 5000000  # how many rows we want to take (to avoid waiting too much for tutorial purposes)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-infection",
   "metadata": {},
   "source": [
    "## Step 1: build a prod2vec space\n",
    "\n",
    "For more information on prod2vec and its use, you can also check our blog post: https://blog.coveo.com/clothes-in-space-real-time-personalization-in-less-than-100-lines-of-code/ or latest NLP paper: https://arxiv.org/abs/2104.02061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sessions_from_training_file(training_file: str, K: int = None):\n",
    "    \"\"\"\n",
    "    Read the training file containing product interactions, up to K rows.\n",
    "    \n",
    "    :return: a list of lists, each list being a session (sequence of product IDs)\n",
    "    \"\"\"\n",
    "    user_sessions = []\n",
    "    current_session_id = None\n",
    "    current_session = []\n",
    "    with open(training_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for idx, row in enumerate(reader):\n",
    "            # if a max number of items is specified, just return at the K with what you have\n",
    "            if K and idx >= K:\n",
    "                break\n",
    "            # just append \"detail\" events in the order we see them\n",
    "            # row will contain: session_id_hash, product_action, product_sku_hash\n",
    "            _session_id_hash = row['session_id_hash']\n",
    "            # when a new session begins, store the old one and start again\n",
    "            if current_session_id and current_session and _session_id_hash != current_session_id:\n",
    "                user_sessions.append(current_session)\n",
    "                # reset session\n",
    "                current_session = []\n",
    "            # check for the right type and append\n",
    "            if row['product_action'] == 'detail':\n",
    "                current_session.append(row['product_sku_hash'])\n",
    "            # update the current session id\n",
    "            current_session_id = _session_id_hash\n",
    "\n",
    "    # print how many sessions we have...\n",
    "    print(\"# total sessions: {}\".format(len(user_sessions)))\n",
    "    # print first one to check\n",
    "    print(\"First session is: {}\".format(user_sessions[0]))\n",
    "    assert user_sessions[0][0] == 'd5157f8bc52965390fa21ad5842a8502bc3eb8b0930f3f8eafbc503f4012f69c'\n",
    "    assert user_sessions[0][-1] == '63b567f4cef976d1411aecc4240984e46ebe8e08e327f2be786beb7ee83216d0'\n",
    "\n",
    "    return user_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_product_2_vec_model(sessions: list,\n",
    "                              min_c: int = 3,\n",
    "                              size: int = 48,\n",
    "                              window: int = 5,\n",
    "                              iterations: int = 15,\n",
    "                              ns_exponent: float = 0.75):\n",
    "    \"\"\"\n",
    "    Train CBOW to get product embeddings. We start with sensible defaults from the literature - please\n",
    "    check https://arxiv.org/abs/2007.14906 for practical tips on how to optimize prod2vec.\n",
    "\n",
    "    :param sessions: list of lists, as user sessions are list of interactions\n",
    "    :param min_c: minimum frequency of an event for it to be calculated for product embeddings\n",
    "    :param size: output dimension\n",
    "    :param window: window parameter for gensim word2vec\n",
    "    :param iterations: number of training iterations\n",
    "    :param ns_exponent: ns_exponent parameter for gensim word2vec\n",
    "    :return: trained product embedding model\n",
    "    \"\"\"\n",
    "    model =  gensim.models.Word2Vec(sentences=sessions,\n",
    "                                    min_count=min_c,\n",
    "                                    vector_size=size,\n",
    "                                    window=window,\n",
    "                                    epochs=iterations,\n",
    "                                    ns_exponent=ns_exponent)\n",
    "\n",
    "    print(\"# products in the space: {}\".format(len(model.wv.index_to_key)))\n",
    "\n",
    "    return model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-strip",
   "metadata": {},
   "source": [
    "Get sessions from the training file, and train a prod2vec model with standard hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sessions\n",
    "sessions = read_sessions_from_training_file(\n",
    "    training_file=os.path.join(LOCAL_FOLDER, 'browsing_train.csv'),\n",
    "    K=N_ROWS)\n",
    "# get a counter on all items for later use\n",
    "sku_cnt = Counter([item for s in sessions for item in s])\n",
    "# print out most common SKUs\n",
    "sku_cnt.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave some sessions aside\n",
    "idx = int(len(sessions) * 0.8)\n",
    "train_sessions = sessions[0: idx]\n",
    "test_sessions = sessions[idx:]\n",
    "print(\"Train sessions # {}, test sessions # {}\".format(len(train_sessions), len(test_sessions)))\n",
    "# finally, train the p2vec, leaving all the default hyperparameters\n",
    "prod2vec_model = train_product_2_vec_model(train_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-scheduling",
   "metadata": {},
   "source": [
    "Show how to get a prediction with knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod2vec_model.similar_by_word(sku_cnt.most_common(1)[0][0], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-stream",
   "metadata": {},
   "source": [
    "Visualize the prod2vec space, color-coding for categories in the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_by_category_with_lookup(title, \n",
    "                                         skus, \n",
    "                                         sku_to_target_cat,\n",
    "                                         results, \n",
    "                                         custom_markers=None):\n",
    "    groups = {}\n",
    "    for sku, target_cat in sku_to_target_cat.items():\n",
    "        if sku not in skus:\n",
    "            continue\n",
    "\n",
    "        sku_idx = skus.index(sku)\n",
    "        x = results[sku_idx][0]\n",
    "        y = results[sku_idx][1]\n",
    "        if target_cat in groups:\n",
    "            groups[target_cat]['x'].append(x)\n",
    "            groups[target_cat]['y'].append(y)\n",
    "        else:\n",
    "            groups[target_cat] = {\n",
    "                'x': [x], 'y': [y]\n",
    "                }\n",
    "    # DEBUG print\n",
    "    print(\"Total of # groups: {}\".format(len(groups)))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    for group, data in groups.items():\n",
    "        ax.scatter(data['x'], data['y'], \n",
    "                   alpha=0.3, \n",
    "                   edgecolors='none', \n",
    "                   s=25, \n",
    "                   marker='o' if not custom_markers else custom_markers,\n",
    "                   label=group)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_analysis(embeddings, perplexity=25, n_iter=1000):\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=perplexity, n_iter=n_iter)\n",
    "    return tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sku_to_category_map(catalog_file, depth_index=1):\n",
    "    \"\"\"\n",
    "    For each SKU, get category from catalog file (if specified)\n",
    "    \n",
    "    :return: dictionary, mapping SKU to a category\n",
    "    \"\"\"\n",
    "    sku_to_cats = dict()\n",
    "    with open(catalog_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            _sku = row['product_sku_hash']\n",
    "            category_hash = row['category_hash']\n",
    "            if not category_hash:\n",
    "                continue\n",
    "            # pick only category at a certain depth in the tree\n",
    "            # e.g. x/xx/xxx, with depth=1, -> xx\n",
    "            branches = category_hash.split('/')\n",
    "            target_branch = branches[depth_index] if depth_index < len(branches) else None\n",
    "            if not target_branch:\n",
    "                continue\n",
    "            # if all good, store the mapping\n",
    "            sku_to_cats[_sku] = target_branch\n",
    "            \n",
    "    return sku_to_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_to_category = get_sku_to_category_map(os.path.join(LOCAL_FOLDER, 'sku_to_content.csv'))\n",
    "print(\"Total of # {} categories\".format(len(set(sku_to_category.values()))))\n",
    "print(\"Total of # {} SKU with a category\".format(len(sku_to_category)))\n",
    "# debug with a sample SKU\n",
    "print(sku_to_category[sku_cnt.most_common(1)[0][0]])\n",
    "skus = prod2vec_model.index_to_key\n",
    "print(\"Total of # {} skus in the model\".format(len(skus)))\n",
    "embeddings = [prod2vec_model[s] for s in skus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out tsne plot with standard params\n",
    "tsne_results = tsne_analysis(embeddings)\n",
    "assert len(tsne_results) == len(skus)\n",
    "plot_scatter_by_category_with_lookup('Prod2vec', skus, sku_to_category, tsne_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a version with only top K categories\n",
    "TOP_K = 5\n",
    "cnt_categories = Counter(list(sku_to_category.values()))\n",
    "top_categories = [c[0] for c in cnt_categories.most_common(TOP_K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out SKUs outside of top categories\n",
    "top_skus = []\n",
    "top_tsne_results = []\n",
    "for _s, _t in zip(skus, tsne_results):\n",
    "    if sku_to_category.get(_s, None) not in top_categories:\n",
    "        continue\n",
    "    top_skus.append(_s)\n",
    "    top_tsne_results.append(_t)\n",
    "# re-plot tsne with filtered SKUs\n",
    "print(\"Top SKUs # {}\".format(len(top_skus)))\n",
    "plot_scatter_by_category_with_lookup('Prod2vec (top {})'.format(TOP_K), \n",
    "                                     top_skus, sku_to_category, top_tsne_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-sodium",
   "metadata": {},
   "source": [
    "### Bonus: faster inference\n",
    "\n",
    "Gensim is awesome and support approximate, faster inference! You need to have installed ANNOY first, e.g. \"pip install annoy\". We re-run here on our prod space the original benchmark for word2vec from gensim!\n",
    "\n",
    "See: https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model and vector that we are using in the comparison\n",
    "annoy_index = AnnoyIndexer(prod2vec_model, 100)\n",
    "test_sku = sku_cnt.most_common(1)[0][0]\n",
    "# test all is good\n",
    "print(prod2vec_model.most_similar([test_sku], topn=2, indexer=annoy_index))\n",
    "print(prod2vec_model.most_similar([test_sku], topn=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_query_time(model, annoy_index=None, queries=5000):\n",
    "    \"\"\"Average query time of a most_similar method over random queries.\"\"\"\n",
    "    total_time = 0\n",
    "    for _ in range(queries):\n",
    "        _v = model[choice(model.index_to_key)]\n",
    "        start_time = time.process_time()\n",
    "        model.most_similar([_v], topn=5, indexer=annoy_index)\n",
    "        total_time += time.process_time() - start_time\n",
    "        \n",
    "    return total_time / queries\n",
    "\n",
    "gensim_time = avg_query_time(prod2vec_model)\n",
    "annoy_time = avg_query_time(prod2vec_model, annoy_index=annoy_index)\n",
    "print(\"Gensim (s/query):\\t{0:.5f}\".format(gensim_time))\n",
    "print(\"Annoy (s/query):\\t{0:.5f}\".format(annoy_time))\n",
    "speed_improvement = gensim_time / annoy_time\n",
    "print (\"\\nAnnoy is {0:.2f} times faster on average on this particular run\".format(speed_improvement))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-warehouse",
   "metadata": {},
   "source": [
    "### Bonus: hyper tuning\n",
    "\n",
    "For more info on hyper tuning in the context of product embeddings, please see our paper: https://arxiv.org/abs/2007.14906 and our data release: https://github.com/coveooss/fantastic-embeddings-sigir-2020.\n",
    "\n",
    "We use the sessions we left out to simulate a small optimization loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_HR_on_NEP(model, sessions, k=10, min_length=3):\n",
    "    _count = 0\n",
    "    _hits = 0\n",
    "    for session in sessions:\n",
    "        # consider only decently-long sessions\n",
    "        if len(session) < min_length:\n",
    "            continue\n",
    "        # update the counter\n",
    "        _count += 1\n",
    "        # get the item to predict\n",
    "        target_item = session[-1]\n",
    "        # get model prediction using before-last item\n",
    "        query_item = session[-2]\n",
    "        # if model cannot make the prediction, it's a failure\n",
    "        if query_item not in model:\n",
    "            continue\n",
    "        predictions = model.similar_by_word(query_item, topn=k)\n",
    "        # debug\n",
    "        # print(target_item, query_item, predictions)\n",
    "        if target_item in [p[0] for p in predictions]:\n",
    "            _hits += 1\n",
    "    # debug\n",
    "    print(\"Total test cases: {}\".format(_count))\n",
    "    \n",
    "    return _hits / _count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we simulate a test with 3 values for epochs in prod2ve\n",
    "iterations_values = [1, 10]\n",
    "# for each value we train a model, and use Next Event Prediction (NEP) to get a quality assessment\n",
    "for i in iterations_values:\n",
    "    print(\"\\n ======> Hyper value: {}\".format(i))\n",
    "    cnt_model = train_product_2_vec_model(train_sessions, iterations=i)\n",
    "    # use hold-out to have NEP performance\n",
    "    _hr = calculate_HR_on_NEP(cnt_model, test_sessions)\n",
    "    print(\"HR: {}\\n\".format(_hr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-fairy",
   "metadata": {},
   "source": [
    "## Step 2: improving low-count vectors\n",
    "\n",
    "For more information about prod2vec in the cold start scenario, please see our paper: https://dl.acm.org/doi/10.1145/3383313.3411477 and video: https://vimeo.com/455641121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mapper(pro2vec_dims=48):\n",
    "    \"\"\"\n",
    "    Build a Keras model for content-based \"fake\" embeddings.\n",
    "    \n",
    "    :return: a Keras model, mapping BERT-like catalog representations to the prod2vec space\n",
    "    \"\"\"\n",
    "    # input\n",
    "    description_input = Input(shape=(50,))\n",
    "    image_input = Input(shape=(50,))\n",
    "    # model\n",
    "    x = Dense(25, activation=\"relu\")(description_input)\n",
    "    y = Dense(25, activation=\"relu\")(image_input)\n",
    "    combined = Concatenate()([x, y])\n",
    "    combined = Dropout(0.3)(combined)\n",
    "    combined = Dense(25)(combined)\n",
    "    output = Dense(pro2vec_dims)(combined)\n",
    "\n",
    "    return Model(inputs=[description_input, image_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectors representing text and images in the catalog\n",
    "def get_sku_to_embeddings_map(catalog_file):\n",
    "    \"\"\"\n",
    "    For each SKU, get the text and image embeddings, as provided pre-computed by the dataset\n",
    "    \n",
    "    :return: dictionary, mapping SKU to a tuple of embeddings\n",
    "    \"\"\"\n",
    "    sku_to_embeddings = dict()\n",
    "    with open(catalog_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            _sku = row['product_sku_hash']\n",
    "            _description = row['description_vector']\n",
    "            _image = row['image_vector']\n",
    "            # skip when both vectors are not there\n",
    "            if not _description or not _image:\n",
    "                continue\n",
    "            # if all good, store the mapping\n",
    "            sku_to_embeddings[_sku] = (json.loads(_description), json.loads(_image))\n",
    "            \n",
    "    return sku_to_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_to_embeddings = get_sku_to_embeddings_map(os.path.join(LOCAL_FOLDER, 'sku_to_content.csv'))\n",
    "print(\"Total of # {} SKUs with embeddings\".format(len(sku_to_embeddings)))\n",
    "# print out an example\n",
    "_d, _i = sku_to_embeddings['438630a8ba0320de5235ee1bedf3103391d4069646d640602df447e1042a61a3']\n",
    "print(len(_d), len(_i), _d[:5], _i[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just make sure we have the SKUs in the model and a counter\n",
    "skus = prod2vec_model.index_to_key\n",
    "print(\"Total of # {} skus in the model\".format(len(skus)))\n",
    "print(sku_cnt.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above which percentile of frequency we consider SKU popular enough to be our training set?\n",
    "FREQUENT_PRODUCTS_PTILE = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "_counts = [c[1] for c in sku_cnt.most_common()]\n",
    "_counts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have just SKUS in the prod2vec space for which we have embeddings\n",
    "popular_threshold = np.percentile(_counts, FREQUENT_PRODUCTS_PTILE)\n",
    "popular_skus = [s for s in skus if s in sku_to_embeddings and sku_cnt.get(s, 0) > popular_threshold]\n",
    "product_embeddings = [prod2vec_model[s] for s in popular_skus]\n",
    "description_embeddings = [sku_to_embeddings[s][0] for s in popular_skus]\n",
    "image_embeddings = [sku_to_embeddings[s][1] for s in popular_skus]\n",
    "# debug\n",
    "print(popular_threshold, len(skus), len(popular_skus))\n",
    "# print(description_embeddings[:1][:3])\n",
    "# print(image_embeddings[:1][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the mapper now\n",
    "training_data_X = [np.array(description_embeddings), np.array(image_embeddings)]\n",
    "training_data_y = np.array(product_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20, restore_best_weights=True)\n",
    "# build and display model\n",
    "rare_net = build_mapper()\n",
    "plot_model(rare_net, show_shapes=True, show_layer_names=True, to_file='rare_net.png')\n",
    "Image('rare_net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-poultry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train!\n",
    "rare_net.compile(loss='mse', optimizer='rmsprop')\n",
    "rare_net.fit(training_data_X, \n",
    "             training_data_y, \n",
    "             batch_size=200, \n",
    "             epochs=20000, \n",
    "             validation_split=0.2, \n",
    "             callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rarest_skus = [_[0] for _ in sku_cnt.most_common()[-500:]]\n",
    "# test_skus = [s for s in rarest_skus if s in sku_to_embeddings]\n",
    "\n",
    "# get to rare vectors\n",
    "test_skus = [s for s in skus if s in sku_to_embeddings and sku_cnt.get(s, 0) < popular_threshold/2]\n",
    "print(len(skus), len(test_skus))\n",
    "# prepare embeddings for prediction\n",
    "rare_description_embeddings = [sku_to_embeddings[s][0] for s in test_skus]\n",
    "rare_image_embeddings = [sku_to_embeddings[s][1] for s in test_skus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embeddings for prediction\n",
    "test_data_X = [np.array(rare_description_embeddings), np.array(rare_image_embeddings)]\n",
    "predicted_embeddings = rare_net.predict(test_data_X)\n",
    "# debug\n",
    "# print(len(predicted_embeddings))\n",
    "# print(predicted_embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9336162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_HR_on_NEP_rare(model, sessions, rare_skus, k=10, min_length=3):\n",
    "    _count = 0\n",
    "    _hits = 0\n",
    "    _rare_hits = 0\n",
    "    _rare_count = 0\n",
    "    for session in sessions:\n",
    "        # consider only decently-long sessions\n",
    "        if len(session) < min_length:\n",
    "            continue\n",
    "        # update the counter\n",
    "        _count += 1\n",
    "        # get the item to predict\n",
    "        target_item = session[-1]\n",
    "        # get model prediction using before-last item\n",
    "        query_item = session[-2]\n",
    "\n",
    "        # if model cannot make the prediction, it's a failure\n",
    "        if query_item not in model:\n",
    "            continue\n",
    "        \n",
    "        # increment counter if rare sku\n",
    "        if query_item in rare_skus:\n",
    "            _rare_count+=1\n",
    "        \n",
    "        predictions = model.similar_by_word(query_item, topn=k)\n",
    "    \n",
    "        # debug\n",
    "        # print(target_item, query_item, predictions)    \n",
    "        if target_item in [p[0] for p in predictions]:\n",
    "            _hits += 1\n",
    "            # track hits if query is rare sku\n",
    "            if query_item in rare_skus:\n",
    "                _rare_hits+=1\n",
    "    # debug\n",
    "    print(\"Total test cases: {}\".format(_count))\n",
    "    print(\"Total rare test cases: {}\".format(_rare_count))\n",
    "    \n",
    "    return _hits / _count, _rare_hits/_rare_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3894f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy of original prod2vec model\n",
    "prod2vec_rare_model = deepcopy(prod2vec_model)\n",
    "# update model with new vectors\n",
    "prod2vec_rare_model.add_vectors(test_skus, predicted_embeddings, replace=True)\n",
    "prod2vec_rare_model.fill_norms(force=True)\n",
    "# check\n",
    "assert np.array_equal(predicted_embeddings[0], prod2vec_rare_model[test_skus[0]])\n",
    "\n",
    "# test new model\n",
    "calculate_HR_on_NEP_rare(prod2vec_rare_model, test_sessions, test_skus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test original model\n",
    "calculate_HR_on_NEP_rare(prod2vec_model, test_sessions, test_skus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-flour",
   "metadata": {},
   "source": [
    "## Step 3: query scoping\n",
    "\n",
    "For more information about query scoping, please see our paper: https://www.aclweb.org/anthology/2020.ecnlp-1.2/ and repository: https://github.com/jacopotagliabue/session-path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectors representing text and images in the catalog\n",
    "def get_query_to_category_dataset(search_file, cat_2_id, sku_to_category):\n",
    "    \"\"\"\n",
    "    For each query, get a label representing the category in items clicked after the query.\n",
    "    It uses as input a mapping \"sku_to_category\" to join the search file with catalog meta-data!\n",
    "    \n",
    "    :return: two lists, matching query vectors to a label\n",
    "    \"\"\"\n",
    "    query_X = list()\n",
    "    query_Y = list()\n",
    "    with open(search_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            _click_products = row['clicked_skus_hash']\n",
    "            if not _click_products: # or _click_product not in sku_to_category:\n",
    "                continue\n",
    "            # clean the string and extract SKUs from array\n",
    "            cleaned_skus = ast.literal_eval(_click_products)\n",
    "            for s in cleaned_skus: \n",
    "                if s in sku_to_category:\n",
    "                    query_X.append(json.loads(row['query_vector']))\n",
    "                    target_category_as_int = cat_2_id[sku_to_category[s]]\n",
    "                    query_Y.append(utils.to_categorical(target_category_as_int, num_classes=len(cat_2_id)))\n",
    "            \n",
    "    return query_X, query_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-wings",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sku_to_category = get_sku_to_category_map(os.path.join(LOCAL_FOLDER, 'sku_to_content.csv'))\n",
    "print(\"Total of # {} categories\".format(len(set(sku_to_category.values()))))\n",
    "cats = list(set(sku_to_category.values()))\n",
    "cat_2_id = {c: idx for idx, c in enumerate(cats)}\n",
    "print(cat_2_id[cats[0]])\n",
    "query_X, query_Y = get_query_to_category_dataset(os.path.join(LOCAL_FOLDER, 'search_train.csv'), \n",
    "                                                 cat_2_id,\n",
    "                                                 sku_to_category)\n",
    "print(len(query_X))\n",
    "print(query_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(query_X), np.array(query_Y), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_scoping_model(input_d, target_classes):\n",
    "    print('Shape tensor {}, target classes {}'.format(input_d, target_classes))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_d))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(target_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model = build_query_scoping_model(x_train[0].shape[0], y_train[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "query_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# train first\n",
    "query_model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "# compute and print eval score\n",
    "score = query_model.evaluate(x_test, y_test, batch_size=32)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectors representing text and images in the catalog\n",
    "def get_query_info(search_file):\n",
    "    \"\"\"\n",
    "    For each query, extract relevant metadata of query and to match with session data\n",
    "\n",
    "    :return: list of queries with metadata\n",
    "    \"\"\"\n",
    "    queries = list()\n",
    "    with open(search_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            _click_products = row['clicked_skus_hash']\n",
    "            if not _click_products: # or _click_product not in sku_to_category:\n",
    "                continue\n",
    "            # clean the string and extract SKUs from array\n",
    "            cleaned_skus = ast.literal_eval(_click_products)\n",
    "            queries.append({'session_id_hash' : row['session_id_hash'],\n",
    "                            'server_timestamp_epoch_ms' : int(row['server_timestamp_epoch_ms']),\n",
    "                            'clicked_skus' : cleaned_skus,\n",
    "                            'query_vector' : json.loads(row['query_vector'])})\n",
    "    print(\"# total queries: {}\".format(len(queries)))        \n",
    "    \n",
    "    return queries\n",
    "\n",
    "def get_session_info_for_queries(training_file: str, query_info: list, K: int = None):\n",
    "    \"\"\"\n",
    "    Read the training file containing product interactions for sessions with query, up to K rows.\n",
    "    \n",
    "    :return: dict of lists with session_id as key, each list being a session (sequence of product events with metadata) \n",
    "    \"\"\"\n",
    "    user_sessions = dict()\n",
    "    current_session_id = None\n",
    "    current_session = []\n",
    "    \n",
    "    query_session_ids = set([ _['session_id_hash'] for _ in query_info])\n",
    "\n",
    "    with open(training_file) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for idx, row in enumerate(reader):\n",
    "            # if a max number of items is specified, just return at the K with what you have\n",
    "            if K and idx >= K:\n",
    "                break\n",
    "            # just append \"detail\" events in the order we see them\n",
    "            # row will contain: session_id_hash, product_action, product_sku_hash\n",
    "            _session_id_hash = row['session_id_hash']\n",
    "            # when a new session begins, store the old one and start again\n",
    "            if current_session_id and current_session and _session_id_hash != current_session_id:\n",
    "                user_sessions[current_session_id] = current_session\n",
    "                # reset session\n",
    "                current_session = []\n",
    "            # check for the right type and append event info\n",
    "            if row['product_action'] == 'detail' and _session_id_hash in query_session_ids :\n",
    "                current_session.append({'product_sku_hash': row['product_sku_hash'],\n",
    "                                        'server_timestamp_epoch_ms' : int(row['server_timestamp_epoch_ms'])})\n",
    "            # update the current session id\n",
    "            current_session_id = _session_id_hash\n",
    "\n",
    "    # print how many sessions we have...\n",
    "    print(\"# total sessions: {}\".format(len(user_sessions)))\n",
    "\n",
    "\n",
    "    return dict(user_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2dc83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_info = get_query_info(os.path.join(LOCAL_FOLDER, 'search_train.csv'))\n",
    "session_info = get_session_info_for_queries(os.path.join(LOCAL_FOLDER, 'browsing_train.csv'), query_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b2172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextual_query_to_category_dataset(query_info, session_info, prod2vec_model, cat_2_id, sku_to_category):\n",
    "    \"\"\"\n",
    "    For each query, get a label representing the category in items clicked after the query.\n",
    "    It uses as input a mapping \"sku_to_category\" to join the search file with catalog meta-data!\n",
    "    It also creates a joint embedding for input by concatenating query vector and average session vector up till\n",
    "    when query was made\n",
    "    \n",
    "    :return: two lists, matching query vectors to a label\n",
    "    \"\"\"\n",
    "    query_X = list()\n",
    "    query_Y = list()\n",
    "    \n",
    "    for row in query_info:\n",
    "        query_timestamp = row['server_timestamp_epoch_ms']\n",
    "        cleaned_skus = row['clicked_skus']\n",
    "        session_id_hash = row['session_id_hash']\n",
    "        if session_id_hash not in session_info or not cleaned_skus: # or _click_product not in sku_to_category:\n",
    "            continue            \n",
    "            \n",
    "        session_skus = session_info[session_id_hash]\n",
    "        context_skus = [ e['product_sku_hash'] for e in session_skus if query_timestamp > e['server_timestamp_epoch_ms'] \n",
    "                                                                        and e['product_sku_hash'] in prod2vec_model]\n",
    "        if not context_skus:\n",
    "            continue\n",
    "        context_vector = np.mean([prod2vec_model[sku] for sku in context_skus], axis=0).tolist()\n",
    "        for s in cleaned_skus: \n",
    "            if s in sku_to_category:\n",
    "                query_X.append(row['query_vector'] + context_vector)\n",
    "                target_category_as_int = cat_2_id[sku_to_category[s]]\n",
    "                query_Y.append(utils.to_categorical(target_category_as_int, num_classes=len(cat_2_id)))\n",
    "            \n",
    "    return query_X, query_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_query_X, context_query_Y = get_contextual_query_to_category_dataset(query_info, \n",
    "                                                                            session_info, \n",
    "                                                                            prod2vec_model, \n",
    "                                                                            cat_2_id, \n",
    "                                                                            sku_to_category)\n",
    "print(len(context_query_X))\n",
    "print(context_query_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c7429",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(context_query_X), np.array(context_query_Y), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbdba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "contextual_query_model = build_query_scoping_model(x_train[0].shape[0], y_train[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "contextual_query_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# train first\n",
    "contextual_query_model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "# compute and print eval score\n",
    "score = contextual_query_model.evaluate(x_test, y_test, batch_size=32)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14715b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef8783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f7f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
